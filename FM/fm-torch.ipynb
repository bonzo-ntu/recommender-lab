{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/github/recommender-lab/FM')\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import d2l\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, num_embeddings:int, embedding_dim:int):\n",
    "        \"\"\"\n",
    "        num_embeddings: number of categories, which is the dimension of one-hot encoding vector\n",
    "        embedding_dim: dimension of each embedding vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        num_embeddings = int(sum(num_embeddings))\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc = nn.Embedding(num_embeddings, 1) # 輸出維度只有 1 維\n",
    "        self.linear_layer = nn.Linear(1, 1)\n",
    "        \n",
    "        # Initialize weights using Xavier Uniform initialization\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "        init.xavier_uniform_(self.fc.weight)\n",
    "        init.xavier_uniform_(self.linear_layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        square_of_sum = torch.sum(self.embedding(x), axis=1) ** 2 #np.sum(self.embedding(x), axis=1) ** 2\n",
    "        sum_of_square = torch.sum(self.embedding(x) ** 2, axis=1)\n",
    "        x = self.linear_layer(self.fc(x).sum(1)) + 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, data_path='../data/ctr/train.csv', feat_mapper=None, defaults=None,\n",
    "                 min_threshold=4, num_feat=34):\n",
    "        self.NUM_FEATS, self.count, self.data = num_feat, 0, {}\n",
    "        feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "        self.feat_mapper, self.defaults = feat_mapper, defaults\n",
    "        self.field_dims = np.zeros(self.NUM_FEATS, dtype=np.int64)\n",
    "        with open(data_path) as f:\n",
    "            for line in f:\n",
    "                instance = {}\n",
    "                values = line.rstrip('\\n').split('\\t')\n",
    "                if len(values) != self.NUM_FEATS + 1:\n",
    "                    continue\n",
    "                label = np.float32([0, 0])\n",
    "                label[int(values[0])] = 1\n",
    "                instance['y'] = [np.float32(values[0])]\n",
    "                for i in range(1, self.NUM_FEATS + 1):\n",
    "                    feat_cnts[i][values[i]] += 1\n",
    "                    instance.setdefault('x', []).append(values[i])\n",
    "                self.data[self.count] = instance\n",
    "                self.count = self.count + 1\n",
    "\n",
    "\n",
    "        if self.feat_mapper is None and self.defaults is None:\n",
    "            feat_mapper = {i: {feat for feat, c in cnt.items() if c >=\n",
    "                               min_threshold} for i, cnt in feat_cnts.items()}\n",
    "            self.feat_mapper = {i: {feat_v: idx for idx, feat_v in enumerate(feat_values)}\n",
    "                                for i, feat_values in feat_mapper.items()}\n",
    "            self.defaults = {i: len(feat_values) for i, feat_values in feat_mapper.items()}\n",
    "\n",
    "        for i, fm in self.feat_mapper.items():\n",
    "            self.field_dims[i - 1] = len(fm) + 1\n",
    "        #self.offsets = np.array((0, *np.cumsum(self.field_dims).asnumpy()[:-1]))\n",
    "        self.offsets = np.array((0, *np.cumsum(self.field_dims)[:-1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = np.array([self.feat_mapper[i + 1].get(v, self.defaults[i + 1])\n",
    "                         for i, v in enumerate(self.data[idx]['x'])])\n",
    "        return feat + self.offsets, self.data[idx]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "train_data = CTRDataset('../data/ctr/train.csv')\n",
    "test_data = CTRDataset('../data/ctr/test.csv',\n",
    "                       feat_mapper=train_data.feat_mapper,\n",
    "                       defaults=train_data.defaults)\n",
    "\n",
    "train_iter = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "test_iter = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = torch.device(\"cuda:0\")\n",
    "net = FM(train_data.field_dims, embedding_dim=20)\n",
    "lr, num_epochs = 0.02, 30\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "def accuracy(pred, target):\n",
    "    result = pred.round() == target\n",
    "    return result.sum(), result.shape[0]\n",
    "\n",
    "# trainer = gluon.Trainer(net.parameters(), optimizer,\n",
    "#                         {'learning_rate': lr})\n",
    "# loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "# d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] training loss: 0.839, training acc:0.5712666511535645, validation loss:1.347, validation acc:0.7329999804496765\n",
      "[2] training loss: 0.695, training acc:0.8594666719436646, validation loss:1.133, validation acc:0.9103333353996277\n",
      "[3] training loss: 0.617, training acc:0.9380000233650208, validation loss:1.082, validation acc:0.9303333163261414\n",
      "[4] training loss: 0.601, training acc:0.9574666619300842, validation loss:1.070, validation acc:0.9319999814033508\n",
      "[5] training loss: 0.593, training acc:0.9681333303451538, validation loss:1.067, validation acc:0.9306666851043701\n",
      "[6] training loss: 0.589, training acc:0.977066695690155, validation loss:1.072, validation acc:0.9306666851043701\n",
      "[7] training loss: 0.587, training acc:0.9803333282470703, validation loss:1.075, validation acc:0.9296666383743286\n",
      "[8] training loss: 0.586, training acc:0.982866644859314, validation loss:1.070, validation acc:0.9300000071525574\n",
      "[9] training loss: 0.583, training acc:0.9873999953269958, validation loss:1.072, validation acc:0.9303333163261414\n",
      "[10] training loss: 0.582, training acc:0.9889333248138428, validation loss:1.072, validation acc:0.9283333420753479\n",
      "[11] training loss: 0.582, training acc:0.9897333383560181, validation loss:1.073, validation acc:0.9276666641235352\n",
      "[12] training loss: 0.580, training acc:0.9911999702453613, validation loss:1.078, validation acc:0.9266666769981384\n",
      "[13] training loss: 0.580, training acc:0.9917333126068115, validation loss:1.073, validation acc:0.9286666512489319\n",
      "[14] training loss: 0.580, training acc:0.9926000237464905, validation loss:1.068, validation acc:0.9283333420753479\n",
      "[15] training loss: 0.580, training acc:0.9933333396911621, validation loss:1.082, validation acc:0.9273333549499512\n",
      "[16] training loss: 0.579, training acc:0.9936000108718872, validation loss:1.082, validation acc:0.9259999990463257\n",
      "[17] training loss: 0.578, training acc:0.9941333532333374, validation loss:1.068, validation acc:0.9279999732971191\n",
      "[18] training loss: 0.579, training acc:0.9944000244140625, validation loss:1.071, validation acc:0.9276666641235352\n",
      "[19] training loss: 0.578, training acc:0.9947999715805054, validation loss:1.067, validation acc:0.9279999732971191\n",
      "[20] training loss: 0.578, training acc:0.9949333071708679, validation loss:1.074, validation acc:0.9279999732971191\n",
      "[21] training loss: 0.578, training acc:0.9952666759490967, validation loss:1.081, validation acc:0.9290000200271606\n",
      "[22] training loss: 0.578, training acc:0.9954666495323181, validation loss:1.075, validation acc:0.9273333549499512\n",
      "[23] training loss: 0.578, training acc:0.9955333471298218, validation loss:1.069, validation acc:0.9286666512489319\n",
      "[24] training loss: 0.576, training acc:0.9957333207130432, validation loss:1.080, validation acc:0.9293333292007446\n",
      "[25] training loss: 0.577, training acc:0.9958666563034058, validation loss:1.079, validation acc:0.9279999732971191\n",
      "[26] training loss: 0.577, training acc:0.9959999918937683, validation loss:1.062, validation acc:0.9273333549499512\n",
      "[27] training loss: 0.578, training acc:0.996066689491272, validation loss:1.085, validation acc:0.9273333549499512\n",
      "[28] training loss: 0.577, training acc:0.996066689491272, validation loss:1.074, validation acc:0.9276666641235352\n",
      "[29] training loss: 0.577, training acc:0.996066689491272, validation loss:1.072, validation acc:0.9273333549499512\n",
      "[30] training loss: 0.576, training acc:0.9961333274841309, validation loss:1.073, validation acc:0.9286666512489319\n"
     ]
    }
   ],
   "source": [
    "# animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    acc, n = 0.0, 0\n",
    "    for i, (inputs, labels) in enumerate(train_iter):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels[0], 1))\n",
    "        matchs, ns = accuracy(outputs, torch.unsqueeze(labels[0], 1))\n",
    "        acc += matchs\n",
    "        n += ns\n",
    "\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    acc2, n2 = 0.0, 0\n",
    "    for i2, (inputs2, labels2) in enumerate(test_iter):\n",
    "        outputs2 = net(inputs2)\n",
    "        vloss = criterion(outputs2, torch.unsqueeze(labels2[0], 1))\n",
    "        matchs2, ns2 = accuracy(outputs2, torch.unsqueeze(labels2[0], 1))\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "        acc2 += matchs2\n",
    "        n2 += ns2\n",
    "        \n",
    "\n",
    "    # print every 2000 mini-batches\n",
    "    print(f'[{epoch+1}] training loss: {running_loss/i:.3f}, training acc:{acc/n}, validation loss:{running_vloss/i2:.3f}, validation acc:{acc2/n2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
